Limitations of Gen1
====
1. Single Namespace --> Single NN
2. NN --> SPOF  --> No HighAvailability
3. Single JobTracker
4. Only MR Framework to process the data


Limitation Number 1 of Gen1 Taken care by Gen2
==============================================
1. Single NN running and managing Single Namespace.  Maintains metadata in RAM
   Secondary NN --> Taking backup and reg intervals  --> Default 1 hour

100 slaves/ 1000 slaves  --> Managed by Single NN

Max tested till --> 4000 servers --> Single NN --> Single NameSpace

User data  --> One NN for user data
IT data    --> One NN for IT data
hbase data --> One NN for hbase data

Problem
Namespace depends on size of RAM of NN

Solution
==========
Federation :  Multiple NN managing different Namespaces  --> Big cluster  > 4000 servers

Limitation Number 2 of Gen1 Taken care by Gen2
==============================================
1. If you loose Namenode you will loose the Cluster details
2. Manual intervention should be there to start new NameNode and copy backup from SecondaryNN

Problem

10am --> backup to SNN  
10:45am --> NN breakdown  --> You can get data till 10:00am from SNN ( Problem in Gen 1 )

Solution
==========
HighAvailability :  Active and Standby Namenodes manage samedata at given point of time.
--> In case Active NN fails Standby NN will act as Active and serves request


Limitation Number 3 of Gen1 Taken care by Gen2
==============================================
Sinble JobTracker to manage thousands of jobs

Problem

Jobtracker was overburdend 

Solution
==========
YARN with multiple deamons like ResourceManager, NodeManger, ApplicationMaster(one per Application)

Container  --> variable resources allocated per task(in slave m/c) --> cpu,memory,disk,network

1. Resource Manager --> Entire Cluster Lever
2. NodeManager      --> Per Node/Slave/machine/server
3. App Master       --> life cycle of job  ( App Master one per job )




slave
=====
slave1 or ip address ( 192.168.1.25)
slave2 or ip address ( 192.168.1.26)
slave3 or ip address ( 192.168.1.27)
salve4 or ip address ( 192.168.1.28)



FUSE of facebook
setrep






Basic flow of MR job 
=====================
1. Client communicates with RM to initiate process.
2. RM response with app ID.
3. Client create "App Submit context"  ( App jar file, resource required, Secu tokens, Schedular)
   and submit to RM
4. RM request NodeManager to start container for AppMaster
5. App Master Register itself with RM
6. RM conveys details for starting respective app to App Master
7. App Master req respective NodeManager to start task ( by allocating contaiers )


EMP.dat 100 mb ( 10000 recs )

1. we copy file to hadoop

64mb --> 2 blocks with 3 repl --> 6 blocks for emp.dat --> distributed entire cluster


2 blocks  --> 2 Map task


Block1 --> 7000 recs  --> map1 --> 2000
Block2 --> 3000 recs  --> map2 --> 1000


1. Find how many emp get sal > 40000

MapReduce program 

1. Map

   a. Reads data from input files
   b. Implement Business Logic --> Developer  --> check sal> 40000 if yes incr counter
   c. Output --> counter

Maps output
===========
map1
2000

map2
1000

2. Reduce

	a. Output of all maps will be given to Reducer by hadoop
        b. Aggregation logic 
        c. Output

Reducer output
=============

3000


1. Create jar file of these classes 

2. run jar on input data


/usr/lib/hadoop-2.2.0 --> Hadoop_Home

Hadoop_Home/etc/hadoop  --> Configuration files

Hadoop_Home/share/hadoop --> jar files


1. Create a toy data
2. Copy this data to hadoop
3. Run job ( wordcount )

hadoop jar <jar_filename> <prg_name> <input_file(hdfs path)> <output_dir(hdfs path)>


All deamon running in a single machine --> Pseudo Distributed Mode

Multiple Machines --> Fully Distributed Mode


put --> copyFromLocal --> copy files from LFS to HDFS
get --> copyToLocal   --> copy files from HDFS to LFS



1. install linux-server , java , hadoop in all machines
2. Choose any one server/system go to hadoop home/etc/hadoop to change conf file
3. copy this folder to all servers
4. start hadoop




















=====
SKU Forecasting

1000 retail outlets data --> 10 am 






